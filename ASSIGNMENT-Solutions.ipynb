{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 2 - IDC 409\n",
    "## CLASSIFYING MNIST DATASET BY ARTIFICIAL NEURAL NETWORKS INVOLVING -\n",
    "* NO HIDDEN LAYER <BR>\n",
    "* SINGLE HIDDEN LAYER <BR>\n",
    "* TWO HIDDEN LAYERS <BR>\n",
    "VIA THE FOLLOWING ACTIVATION FUNCTIONS - <BR>\n",
    "* SIGMOID\n",
    "* TANH\n",
    "* RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt \n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    Y=[]\n",
    "    X=[]\n",
    "    #path='/Users/sanatmishra27/Downloads/mnist_train.csv'\n",
    "    data = np.loadtxt(path,delimiter=\",\")\n",
    "    for row in data:\n",
    "        X.append(row[:-1])\n",
    "        Y.append(int(row[-1]))\n",
    "    return np.array(X),Y\n",
    "\n",
    "data=load_data('/Users/sanatmishra27/Downloads/mnist_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we move onto the rest of the problems, we do the following- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is first preprocessed to ensure only non-zero pixel values for each of the 10,000 samples are contained so as to update weights without errors.<br>\n",
    "Thus we shall scale down all values to a range of [0.01,1] by multiplying by a factor and adding 0.01 to remove zero valued entries that obstruct updating weights. <br>\n",
    "Finally, we one-hot encode the class labels. <br>\n",
    "Note that we have taken transposes of matrices appropriately at each step to ensure valid multiplication of matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac =  0.99/255\n",
    "x_train = np.asfarray(data[0][:, 0:])*fac + 0.01\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, data[1],test_size=0.3,random_state=0,stratify=data[1])\n",
    "'''Stratify was used to split the data while testing such that it created training in the same proportion of \n",
    "labels as is present in the original data. '''\n",
    "\n",
    "#One Hot Encoding the labels\n",
    "n_values = int(np.max(data[1])+1)\n",
    "y_train=np.eye(n_values)[data[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's proceed to first initialize a weight matrix and a bias matrix. These two matrices are very critical to the performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) WEIGHT MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights have been initialised by constructing a matrix whose values are picked from a standard normal curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=x_train.shape[1]\n",
    "output=10\n",
    "w= (np.random.randn(10,input))\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) BIAS MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias helps in controlling the value at which activation function (in this case the softmax function) will trigger. Thus it helps in quickening the search for the best-fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(10, x_train.shape[0])  \n",
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compiling all the above information into a zero hidden layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def feedforward(b,w):\n",
    "    z = np.dot(w,x_train.T)+b\n",
    "    return z   # z is a (10x7000) matrix\n",
    "\n",
    "def softmax(s):\n",
    "    exps = np.exp(s-np.max(s)) # We subtract the additional term to ensure values don't greatly shoot up.\n",
    "    return exps/np.sum(exps, axis=0, keepdims=True)\n",
    "\n",
    "def errordiff(pred, real):\n",
    "    samplesize = real.shape[0]\n",
    "    res = pred - real\n",
    "    return res /samplesize\n",
    "\n",
    "def cross_entropy(predict,real):\n",
    "    samplesize = real.shape[0] #7,000\n",
    "    L_sum = np.sum(np.dot(np.log(predict),real))\n",
    "    L = -(1/samplesize) * L_sum\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few points to note in the above, the softmax function is subtracted by its maximum value to ensure that values don't become extraordinarily big. <br>\n",
    "The error function returns the diffeence scaled by the sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the function below computes and returns the updated weights and biases. <br>\n",
    "The list of matrices used in the following code, along with their dimensions are as follows - <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) x_train (10000x784) <br>\n",
    "2) y_train (10000x10) <br>\n",
    "3) delta (10000x10) <br>\n",
    "4) w (10x784) <br>\n",
    "5) b (10x10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now code for the backprop. section and update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updates weights are [[-0.89406232 -0.58249404 -1.3651342  ... -0.62042341 -0.86671015\n",
      "  -0.1765741 ]\n",
      " [-1.64467576  1.48308452  1.89171623 ... -0.01678469  0.47459725\n",
      "  -0.51772445]\n",
      " [-1.51521819 -0.8424277  -0.92399453 ...  0.37174272  1.05012878\n",
      "   0.32129177]\n",
      " ...\n",
      " [ 0.72752567 -0.39740686 -1.29129193 ... -0.40021915 -0.52782602\n",
      "   0.30886132]\n",
      " [-0.64961018 -0.31653074  1.58863764 ... -0.50497616 -0.97614895\n",
      "  -0.58987728]\n",
      " [ 0.42453968 -0.47077215 -0.10170718 ... -0.67563836 -1.80365163\n",
      "  -0.23404338]]\n",
      "The updates biases are [[-1.53750397 -0.72439772  0.03050305 ...  0.49751233  0.50597019\n",
      "  -0.56982844]\n",
      " [ 1.20582748  0.23832829  0.70772384 ... -0.28959508 -1.43657412\n",
      "  -0.15519177]\n",
      " [ 1.17012055  0.35039404 -0.69412379 ... -0.17447291  0.53454542\n",
      "   1.36785118]\n",
      " ...\n",
      " [-0.72156778 -1.59744124 -0.91651469 ...  0.45359374 -0.09664347\n",
      "   0.58468342]\n",
      " [-1.31282311 -1.2114665  -1.69700858 ... -1.72157437 -1.17675753\n",
      "  -0.73309454]\n",
      " [ 1.26131524  1.58322103 -0.0563262  ...  0.09083853  0.65535695\n",
      "  -1.161844  ]]\n"
     ]
    }
   ],
   "source": [
    "def finalbackprop(X,Y,w,b,lr):\n",
    "    delta = errordiff(X.T, Y) \n",
    "    w -= lr * np.dot(delta.T,x_train) \n",
    "    b -= lr * np.sum(delta.T, axis=0, keepdims=True) \n",
    "    return w,b\n",
    "\n",
    "X=softmax(feedforward(b,w)) # X is a (10x7000) matrix\n",
    "Y=y_train  # Y is a (7000x10) matrix\n",
    "\n",
    "results=finalbackprop(X,Y,w,b,0.001)\n",
    "print('The updates weights are',results[0])\n",
    "print('The updates biases are',results[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we code for the single hidden layer. By adjusting the value of N, we can control the number of neurons in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20     # neurons for hidden layers\n",
    "lr = 0.01  # learning rate\n",
    "\n",
    "w1 = np.random.randn(x_train.shape[1], N) \n",
    "b1 = np.random.randn(1,N) \n",
    "w2 = np.random.randn(N, 10)\n",
    "b2 = np.random.randn(1, 10)\n",
    "\n",
    "w=[w1,w2]\n",
    "b=[b1,b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define functions that we invoke along with the feedforward steps-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/(1 + np.exp(-s))\n",
    "\n",
    "def softmax(s):\n",
    "    exps = np.exp(s-np.max(s)) \n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def sigmoid_derv(x):\n",
    "    return x * (1 - x)  \n",
    "\n",
    "def errordiff(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    res = pred - real\n",
    "    return res/n_samples\n",
    "\n",
    "def feedforward(x,w,b):\n",
    "    z1 = np.dot(x,w[0]) + b[0]\n",
    "    a1 = sigmoid(z1) \n",
    "    z2 = np.dot(a1,w[1]) + b[1]\n",
    "    a2 = softmax(z2)  \n",
    "    return a1,a2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finlly, we code the backpropagation section, including returning updated weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updates weights are [array([[-0.31252249,  0.09177449,  0.17649915, ..., -0.76956261,\n",
      "         0.86953219,  0.08318242],\n",
      "       [-2.38953646, -1.24360515,  0.6354934 , ...,  2.33701497,\n",
      "         0.65364067, -0.52491783],\n",
      "       [ 0.73230949,  0.02805841, -0.75257765, ...,  1.35439344,\n",
      "         0.78540072, -1.10121413],\n",
      "       ...,\n",
      "       [ 0.13364363,  0.94423892, -0.01708292, ..., -1.13997884,\n",
      "         0.37784275, -0.57784318],\n",
      "       [ 0.88501366, -0.26987159, -0.14471453, ...,  0.34333959,\n",
      "         1.57944757, -0.76996534],\n",
      "       [-0.81291261, -0.74370468,  1.2644272 , ..., -0.78094806,\n",
      "         0.63357681,  1.04501222]]), array([[-6.88260965e-01,  4.48675625e-01,  1.53502929e+00,\n",
      "        -7.08049016e-02, -5.78452013e-01, -1.21458012e+00,\n",
      "        -1.54421432e-01, -9.43979325e-01,  1.03728750e-01,\n",
      "         1.96380982e-01],\n",
      "       [ 1.86187298e+00,  1.31730479e+00, -3.04355811e-01,\n",
      "        -9.93018940e-01, -1.32333798e+00,  1.92873247e+00,\n",
      "         7.29098895e-02, -1.02233608e+00,  7.55633774e-01,\n",
      "        -2.90279194e-01],\n",
      "       [-1.61804472e+00,  4.34485568e-01, -4.36489799e-01,\n",
      "        -6.61569738e-02, -5.40763096e-01, -2.30330183e+00,\n",
      "         7.70574843e-02,  6.73001807e-01,  4.42890159e-01,\n",
      "         5.92805634e-01],\n",
      "       [-1.78244304e-01,  5.18796901e-01,  3.80217898e-01,\n",
      "         7.32718316e-01,  2.03840297e-01, -1.01578930e+00,\n",
      "        -9.94212423e-01, -1.86870509e+00, -7.24040929e-01,\n",
      "        -1.41210104e+00],\n",
      "       [-2.22177709e-01,  1.17524620e-01, -3.78525561e-01,\n",
      "        -5.06593470e-03, -5.25249228e-01,  6.90078807e-01,\n",
      "         3.65882835e-01, -1.22233781e+00, -8.45996984e-01,\n",
      "         7.12813519e-01],\n",
      "       [ 7.21385257e-01, -3.11014553e-01, -2.24039347e-01,\n",
      "        -8.08690759e-01,  1.23344560e+00,  1.47887062e+00,\n",
      "         1.54225084e+00, -1.30438128e+00, -1.34534364e+00,\n",
      "        -1.90212967e+00],\n",
      "       [-1.31819823e-01,  5.34021624e-01, -1.47277197e+00,\n",
      "        -5.45793370e-01,  9.47321360e-01,  1.85766128e-02,\n",
      "        -6.37438611e-01,  6.00401459e-01, -1.02700668e+00,\n",
      "        -2.19807134e+00],\n",
      "       [ 4.17363875e-01,  8.55022789e-01, -1.00361317e+00,\n",
      "         1.55507399e+00, -6.70092931e-01, -1.29813616e+00,\n",
      "        -1.41238565e-01, -3.30712096e-01,  2.00471911e+00,\n",
      "        -1.12845323e+00],\n",
      "       [-1.83996696e-01,  9.60234047e-01,  1.55861910e+00,\n",
      "         9.64085952e-01,  1.86485970e-01,  2.21553612e+00,\n",
      "        -7.67539810e-01,  8.94553037e-01, -2.08921419e+00,\n",
      "        -7.02213801e-01],\n",
      "       [ 3.64668616e-01, -1.31855626e+00,  7.72444260e-01,\n",
      "         1.58652560e-01, -1.76966420e-01, -6.66098725e-01,\n",
      "         1.13467310e+00,  1.19364723e+00,  1.11330405e+00,\n",
      "        -4.32935689e-01],\n",
      "       [-8.73942034e-01, -1.10732781e+00,  1.11370313e-02,\n",
      "         3.97712621e-01, -7.46909762e-02,  6.85924725e-01,\n",
      "         1.22287316e+00, -1.22133437e+00,  1.56846551e+00,\n",
      "        -6.53728659e-01],\n",
      "       [-1.12730828e+00,  2.53438962e-01, -4.19074414e-01,\n",
      "         5.79537976e-01, -3.04912580e-01, -3.52122478e-01,\n",
      "        -2.00993993e+00,  3.56773696e-01, -2.50999057e+00,\n",
      "        -3.53821017e-01],\n",
      "       [ 6.62244936e-01, -2.66430486e+00, -2.24919035e-01,\n",
      "        -1.46605042e-01, -8.44567983e-01, -1.57323710e-01,\n",
      "        -2.64104887e-01, -5.91918082e-01, -6.70242900e-01,\n",
      "        -2.24652105e-01],\n",
      "       [ 1.57413177e+00, -8.41825002e-02, -1.71347301e+00,\n",
      "         2.82053107e-01,  9.18252581e-01, -3.71398757e-01,\n",
      "         7.63718244e-01,  1.37357961e+00,  2.40738182e-01,\n",
      "        -1.29351203e+00],\n",
      "       [-1.95368776e-01, -5.22950777e-01,  2.94294921e-01,\n",
      "        -2.19599306e-01,  7.00529112e-01, -7.20373906e-01,\n",
      "        -1.79462407e+00,  1.95532355e+00,  2.88493231e-03,\n",
      "         1.05423669e+00],\n",
      "       [-8.44997118e-01,  4.54788380e-01,  1.01231491e-02,\n",
      "         4.78913337e-01,  3.60759611e-01,  1.57868902e+00,\n",
      "        -1.19010666e-01,  1.55187325e+00, -1.47342898e+00,\n",
      "         1.08897267e+00],\n",
      "       [ 7.88385508e-01, -8.58012613e-02, -1.27859804e+00,\n",
      "        -8.62949687e-01,  4.71329205e-01, -8.31527355e-01,\n",
      "        -1.63456535e-01, -1.44846711e-01, -9.11785375e-01,\n",
      "         1.75772232e-01],\n",
      "       [-1.12150034e+00,  2.54251513e-03, -1.50971125e+00,\n",
      "        -6.14236899e-01,  1.03823820e+00,  2.06786576e-01,\n",
      "        -1.39669385e+00, -4.69475531e-01,  1.27277485e+00,\n",
      "         1.67371969e-01],\n",
      "       [ 3.00613398e-01, -1.12859248e+00,  1.28811069e+00,\n",
      "        -1.04527692e-01,  4.22644320e-01, -3.20280211e-01,\n",
      "        -5.64040806e-01, -1.30936708e+00,  6.57863541e-02,\n",
      "         6.01127236e-01],\n",
      "       [ 8.66260782e-01, -6.60780733e-01, -1.18621239e+00,\n",
      "         1.43360124e+00,  1.18951060e+00,  3.66980069e-01,\n",
      "         3.98177927e-01,  6.10782489e-01,  1.27664506e+00,\n",
      "        -6.39547783e-01]])]\n",
      "The updates biases are [array([[ 0.01586549,  1.606774  , -0.71977748,  0.27561272,  0.92915991,\n",
      "         0.19401653, -1.10974062,  0.31451875, -0.38416819,  0.05923018,\n",
      "         2.99778823,  0.8772589 ,  1.31957612,  0.33205588, -2.52041612,\n",
      "         2.36010743,  0.76142789, -1.32662001,  1.6368522 , -0.64686348]]), array([[-0.47292109, -0.47070339,  0.67186668, -1.2222619 ,  0.68569305,\n",
      "         1.77662251, -1.23620968,  0.48124287, -1.59502413,  1.92906   ]])]\n"
     ]
    }
   ],
   "source": [
    "def backprop(a1,a2,w,b):\n",
    "   \n",
    "    a2_del = errordiff(a2, y_train) \n",
    "    z2_delta = np.dot(a2_del, w[1].T)  \n",
    "    a1_del = z2_delta *sigmoid_derv(a1)   \n",
    "                                                  \n",
    "    w2 = w[1]- lr * np.dot(a1.T, a2_del)\n",
    "    b2 = b[1] - lr * np.sum(a2_del, axis=0)\n",
    "    \n",
    "    w1 = w[0]- lr * np.dot(x_train.T, a1_del)  #X.T is (783x10000)\n",
    "    b1 = b[0] - lr * np.sum(a1_del, axis=0)\n",
    "    \n",
    "    weights=[]\n",
    "    biases=[]\n",
    "    weights.extend([w1,w2])\n",
    "    biases.extend([b1,b2])\n",
    "    \n",
    "    return weights,biases\n",
    "\n",
    "X=feedforward(x_train,w,b) # X is a (10x7000) matrix\n",
    "Y=y_train  # Y is a (7000x10) matrix\n",
    "\n",
    "results=backprop(X[0],X[1],w,b)\n",
    "\n",
    "print('The updates weights are',results[0])\n",
    "print('The updates biases are',results[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4, Problem 5 and Problem 6 are identical, with the latter two being built on this one. <br>\n",
    "This two hidden layers are initialised in the following cell, along with the weight and bias matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10      # neurons for hidden layers\n",
    "lr = 0.001  # learning rate\n",
    "Y=y_train  \n",
    "\n",
    "w1 = np.random.randn(x_train.shape[1], N) \n",
    "b1 = np.random.randn(1,N) \n",
    "w2 = np.random.randn(N, N)\n",
    "b2 = np.random.randn(1, N)\n",
    "w3=  np.random.randn(N, 10)\n",
    "b3 = np.random.randn(1,10)\n",
    "\n",
    "w=[w1,w2,w3]\n",
    "b=[b1,b2,b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/(1 + np.exp(-s))\n",
    "\n",
    "def softmax(s):\n",
    "    exps = np.exp(s-  np.max(s))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def sigmoid_derv(x):\n",
    "    return (x) * (1 - x) \n",
    "\n",
    "def errordiff(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    res = pred - real\n",
    "    return res/n_samples\n",
    "\n",
    "def feedforward(x,weights,biases):\n",
    "    z1 = np.dot(x,weights[0]) + biases[0]\n",
    "    a1 = sigmoid(z1) \n",
    "    \n",
    "    z2 = np.dot(a1,weights[1]) + biases[1]  \n",
    "    a2 = sigmoid(z2)  \n",
    "\n",
    "    z3 = np.dot(a2, weights[2]) + biases[2]  \n",
    "    a3 = softmax(z3) \n",
    "    \n",
    "    return a1,a2,a3   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now code for the backpropagation using the functions defined above. <br>\n",
    "#### R is taken to be the number of rows in the input matrix, earlier it was taken to be 10000 at the start of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updates weights are [array([[ 0.67407409,  0.3757721 ,  0.88690447, ..., -0.77641841,\n",
      "         0.50804385, -0.9045933 ],\n",
      "       [ 1.40062852, -0.51525304, -1.78851204, ...,  1.18770084,\n",
      "         1.51110333, -1.21353052],\n",
      "       [ 1.41437577,  1.19022806,  0.63476196, ...,  0.73935823,\n",
      "         0.72608125, -0.79300185],\n",
      "       ...,\n",
      "       [-1.48310581, -0.87508251,  0.91985754, ..., -1.56455178,\n",
      "         0.37041671,  1.0186037 ],\n",
      "       [-1.53768137,  0.68274765,  0.36013257, ..., -0.18147091,\n",
      "        -1.14677775, -1.06832061],\n",
      "       [ 0.19359093,  1.61302044,  1.7220381 , ...,  0.88082665,\n",
      "        -1.20215715, -0.66464174]]), array([[ 0.29627287,  0.21620461,  0.43517362,  0.18514882, -0.2430146 ,\n",
      "        -1.94519513,  1.1697059 , -1.80090294,  1.70215221,  1.71394306],\n",
      "       [ 0.60994204, -0.82047391,  0.83591908, -0.64554344,  0.01957567,\n",
      "        -0.23470401, -0.10730442, -0.48145732, -0.46224903, -1.51170491],\n",
      "       [ 1.15375297,  0.02431071, -1.07044002,  0.66762256, -1.31662371,\n",
      "         0.54547809,  0.49592089,  1.00341518, -0.81499689,  0.39087198],\n",
      "       [-0.54664042,  1.51357733, -0.68577906,  1.36970585, -0.07977765,\n",
      "        -0.85392785, -0.67940794, -0.29305598,  0.96526204, -0.53544323],\n",
      "       [-0.23885917, -0.25861007,  1.17252608, -0.77919783, -1.38739391,\n",
      "        -1.79670731, -0.48501733, -1.63540289,  1.30826364, -0.06446573],\n",
      "       [-1.01484728,  0.36073365, -1.20823099, -1.11988882,  0.65192779,\n",
      "         0.18481085,  0.06269268,  0.00539571,  2.09130717, -0.70985523],\n",
      "       [ 1.15159208,  0.11099468,  0.69915733, -1.89626938, -0.86929257,\n",
      "        -1.26087124, -1.91875452,  0.04215948,  0.28559154,  0.14457666],\n",
      "       [ 1.37195561, -1.5757504 , -0.30646649,  1.12608778, -0.27888622,\n",
      "        -0.71563346, -0.67863747, -0.23943872,  0.70811824,  2.11576013],\n",
      "       [ 0.41096767,  0.61383642,  0.00608189, -0.75969478, -0.27639512,\n",
      "         1.84540574, -0.7842683 ,  1.28461776, -0.03047329, -2.30737135],\n",
      "       [-0.6722345 ,  1.18092799,  0.35918076,  1.24936387,  1.57578381,\n",
      "        -1.03746449,  0.6127034 , -0.24627321,  1.54535004,  0.74290305]]), array([[-2.79292355e+00, -9.89506188e-01,  2.13610937e-01,\n",
      "        -1.11288010e+00,  6.92202104e-01,  3.53241978e-01,\n",
      "        -3.27920331e-01,  2.88681219e-01,  1.96457869e+00,\n",
      "        -1.35780764e-01],\n",
      "       [-1.89608099e+00,  4.16378244e-01, -7.51857045e-01,\n",
      "         8.14234318e-01, -3.76654933e-01, -1.21438256e+00,\n",
      "        -2.95510002e-01, -9.86313833e-01, -5.91406149e-01,\n",
      "         2.00493022e-01],\n",
      "       [ 1.17147783e+00,  1.35890648e+00,  2.67489407e-02,\n",
      "        -2.01462136e-01, -9.77458312e-01, -1.56750324e+00,\n",
      "         1.43019533e-01,  8.00662774e-01, -2.11207209e-01,\n",
      "        -7.45613776e-01],\n",
      "       [-3.25060133e-01,  1.02724494e+00, -7.11173102e-01,\n",
      "        -9.72695972e-03, -1.53906773e-01, -1.75526041e+00,\n",
      "        -9.78561558e-01,  4.88613300e-02, -9.68897428e-02,\n",
      "        -9.77280253e-01],\n",
      "       [ 1.78310145e+00, -1.63373736e+00,  9.10563518e-01,\n",
      "         4.16153699e-01, -9.05847091e-01,  6.24336034e-02,\n",
      "        -4.03761785e-01, -9.46045072e-01,  9.71382602e-01,\n",
      "        -3.26106698e-01],\n",
      "       [ 3.55668305e-04, -1.17337209e+00, -1.71079882e+00,\n",
      "        -5.98593257e-01,  5.13092584e-01,  4.38345521e-01,\n",
      "        -1.12809839e+00, -7.45894473e-01, -1.07882135e-01,\n",
      "         3.09013184e-01],\n",
      "       [ 1.81875882e-01,  7.22160928e-01,  1.83700983e+00,\n",
      "        -2.28876017e+00,  2.05239904e-01,  1.34864845e+00,\n",
      "         2.56681187e+00, -3.54271345e-01,  5.29350792e-01,\n",
      "         2.05480359e+00],\n",
      "       [ 5.26179731e-01,  1.65087800e+00, -5.77956934e-01,\n",
      "         6.55141008e-01, -1.42817073e+00, -1.55847994e+00,\n",
      "        -8.01086755e-01,  1.22581563e+00, -7.75532085e-01,\n",
      "        -1.56568486e+00],\n",
      "       [ 1.15649626e+00, -8.47971278e-01, -7.07590052e-01,\n",
      "        -1.41488115e-01, -9.97545116e-02,  9.68784464e-01,\n",
      "        -1.19125508e+00, -1.45632462e+00,  9.15854866e-01,\n",
      "         1.05025578e+00],\n",
      "       [-7.85535240e-01,  5.00525490e-01,  4.15556672e-01,\n",
      "         1.55874990e-01,  9.35475894e-01,  1.54583186e+00,\n",
      "        -9.50766731e-01,  7.80808558e-01, -8.90831447e-01,\n",
      "        -9.68725925e-01]])]\n",
      "The updates biases are [array([[-0.38692926,  0.60638728, -1.09444662,  0.94534448,  0.33194695,\n",
      "         0.72510947, -0.14399605,  0.79214447, -0.91189243,  1.03485638]]), array([[ 1.11827858,  0.39974659,  1.12932404, -0.99884775,  0.03364852,\n",
      "         0.47497028,  0.56402455, -0.22714115,  0.38531365, -2.43975577]]), array([[ 0.70685235,  1.14965997, -0.41973896,  1.90651569, -0.76468885,\n",
      "         0.289107  , -0.2180672 , -0.84415191,  0.92625782, -0.0198146 ]])]\n"
     ]
    }
   ],
   "source": [
    "def backprop(a1,a2,a3,weights,biases):\n",
    "   \n",
    "    a3_del = errordiff(a3, y_train)             # a3_del is (Rx10 matrix)\n",
    "    z2_delta = np.dot(a3_del, weights[2].T) # z2_delta is (RxN matrix)\n",
    "        \n",
    "    a2_delta = z2_delta *sigmoid_derv(a2)     # a2_delta is (RxN)\n",
    "    z1_delta = np.dot(a2_delta,weights[1].T)  # z1_delta is (RxN)\n",
    "        \n",
    "    a1_del = z1_delta*sigmoid_derv(a1)      # a1_del is (RxN)\n",
    "    \n",
    "    w3 = weights[2]- lr * np.dot(a2.T, a3_del)                  #w3 is (Nx10)\n",
    "    b3 = biases[2] - lr * np.sum(a3_del, axis=0, keepdims=True) #b3 is (1x10)\n",
    "        \n",
    "    w2 = weights[1]- lr * np.dot(a1.T,a2_delta)                   #w2 is (NxN)\n",
    "    b2 = biases[1]- lr * np.sum(a2_delta, axis=0, keepdims=True)  #b2 is (1x5)\n",
    "        \n",
    "    w1 = weights[0]- lr * np.dot(x_train.T, a1_del)                   #w1 is (784xN)\n",
    "    b1 = biases[0]- lr * np.sum(a1_del, axis=0)                 #b1 is (1xN)\n",
    "        \n",
    "    weights=[]\n",
    "    biases=[]\n",
    "    weights.extend([w1,w2,w3])\n",
    "    biases.extend([b1,b2,b3])\n",
    "        \n",
    "    return weights,biases\n",
    "\n",
    "\n",
    "X=feedforward(x_train,w,b) # X is a (10x7000) matrix\n",
    "\n",
    "results=backprop(X[0],X[1],X[2],w,b)\n",
    "print('The updates weights are',results[0])\n",
    "print('The updates biases are',results[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization remains identical to the previous case, a weight matrix 'w' consisting of three weight parameters and a bias matrix 'b' consisting of three bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20    # neurons for hidden layers\n",
    "lr = 0.01 # learning rate\n",
    "\n",
    "w1 = np.random.randn(x_train.shape[1], N) \n",
    "b1 = np.random.randn(1,N) \n",
    "w2 = np.random.randn(N, N)\n",
    "b2 = np.random.randn(1, N) \n",
    "w3=  np.random.randn(N, 10)\n",
    "b3 = np.random.randn(1,10)\n",
    "\n",
    "w=[w1,w2,w3]\n",
    "b=[b1,b2,b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional functions such as activation functions and derivatives of 'tanh','relu' and 'sigmoid' have been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/(1 + np.exp(-s))\n",
    "\n",
    "def tanh(s):\n",
    "    return np.tanh(s)\n",
    "\n",
    "def relu(s):      \n",
    "    return np.maximum(0,s) \n",
    "\n",
    "def softmax(s):\n",
    "    exps = np.exp(s-  np.max(s))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def tanhder(s):\n",
    "    return (1-s**2)\n",
    "\n",
    "def reluder(s):\n",
    "    return 1.0*(s>0)\n",
    "\n",
    "def sigmoid_derv(x):\n",
    "    return x * (1 - x) \n",
    "\n",
    "def errordiff(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    res = pred - real\n",
    "    return res/n_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, activation inputs and backpropagation are contained in a single function. <br>\n",
    "Once the activation function type has been determined, the input is created by feeding through two steps of activation and one softmax. <br>\n",
    "Then, differential values are calculated using chain rule that will aid in backpropagation. <br>\n",
    "Finally, the updated weights are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updates weights are [array([[ 0.85576027,  0.27512277,  0.28360944, ...,  0.7919832 ,\n",
      "        -1.2618036 ,  0.12917041],\n",
      "       [ 0.07278122, -0.07555369,  0.02920275, ...,  1.42552701,\n",
      "         0.64564324,  0.03746054],\n",
      "       [-0.01230897,  0.54347978,  0.29565185, ...,  0.69903835,\n",
      "         0.27678684, -1.33022671],\n",
      "       ...,\n",
      "       [-0.54596993,  1.10810127, -1.40998812, ..., -0.31291899,\n",
      "         1.18476861,  0.32852765],\n",
      "       [ 0.44568798, -2.63726733, -1.51127398, ..., -1.25564567,\n",
      "         1.01032798,  0.38360471],\n",
      "       [-1.47158202,  1.0667911 , -0.50143151, ..., -0.2640538 ,\n",
      "        -0.93854619,  0.29067035]]), array([[-0.55272297, -0.091603  ,  0.40878732, -0.70904778,  0.49355514,\n",
      "         1.13851314,  0.08785083, -0.15631827, -1.14589406,  0.07756585,\n",
      "        -2.1526645 ,  0.12432614, -2.01753758, -0.83435512,  0.58439715,\n",
      "        -0.25983306, -0.00776084, -0.79064996,  0.85458005,  1.16990901],\n",
      "       [ 0.43669043, -1.14531951,  2.53850692,  2.21452024,  1.17376908,\n",
      "        -0.25513111, -1.4274292 , -0.07728763, -0.0591619 , -1.49738416,\n",
      "         0.93960725, -0.60702947,  1.72750704,  0.42390745, -0.24733905,\n",
      "        -1.01816733,  0.28063463, -0.40759341, -1.2814965 , -0.92167424],\n",
      "       [-0.93212579, -0.08302179, -0.95088025,  1.09095658, -0.3262011 ,\n",
      "         1.178731  , -0.02441633,  1.21827518, -0.85005294,  0.23345477,\n",
      "         0.88810867, -0.27150137,  1.37992176, -0.98815399, -0.4725469 ,\n",
      "         1.74204417,  1.31803163,  0.64124412, -0.20414675,  1.60327593],\n",
      "       [ 0.07093846, -2.30025854, -1.39680056,  0.69307696,  1.91789182,\n",
      "        -0.1370248 , -0.96829435, -0.05585007, -0.29229804, -1.09902539,\n",
      "         0.24884831,  0.17644457,  0.40070409, -0.96541476,  1.80404802,\n",
      "        -0.04619707,  0.63339059, -1.00852942, -0.24435114, -0.66035685],\n",
      "       [ 0.14392593, -0.92693974,  0.54400676,  0.91287073,  0.96419954,\n",
      "        -1.39072398, -0.09709032, -0.42021173,  0.06026981,  0.3008073 ,\n",
      "         1.42305939, -0.26276042,  0.08888479,  0.03866829,  1.58239689,\n",
      "        -1.05078829,  0.14762013,  0.56985005,  0.64641506,  0.88359751],\n",
      "       [ 0.96796111, -1.09898941, -1.26754147,  1.17441538, -0.10215639,\n",
      "        -0.32758706, -0.10902215, -1.2842818 , -0.03510971, -0.58644888,\n",
      "        -0.56669855, -0.54337912,  0.11579941,  0.14577536, -0.78833112,\n",
      "        -1.63917944, -0.92015549,  1.28066138,  0.60004863, -0.48165108],\n",
      "       [ 0.14222274,  1.51621127,  1.58923173,  0.37643927, -0.12810738,\n",
      "         0.4083427 , -0.64610233,  1.10514583, -0.62362306,  0.59668636,\n",
      "        -0.58979933,  1.3787406 ,  0.27342827, -0.28268694,  2.0754334 ,\n",
      "         0.21907576,  1.7059507 ,  1.78639949, -0.33294462, -0.32045291],\n",
      "       [-0.64866346, -0.60887391, -0.68241039, -0.28355839, -0.12067847,\n",
      "         0.84245335, -0.66941023,  0.06671405, -0.55078072, -1.4516982 ,\n",
      "         0.71994939,  2.1425499 , -0.6227538 , -0.91506388,  0.62859563,\n",
      "        -1.78936112,  0.41108822,  0.65641646, -0.48608445, -2.34508762],\n",
      "       [-1.32023157,  0.44737925, -0.89136639, -0.26751443,  0.40461257,\n",
      "         0.14346325, -2.44127579, -1.06544908, -1.06856975,  1.67755396,\n",
      "        -0.04823114, -1.04168344,  0.04560811, -0.13778155, -0.20483273,\n",
      "        -0.90853424, -0.17167163,  0.54114193, -0.64707962, -0.48496914],\n",
      "       [-1.17942891, -0.41219898,  0.11514261,  0.73906896, -0.94015391,\n",
      "         1.11335633, -0.48305065, -0.19320874, -0.22157481, -0.7618164 ,\n",
      "         0.94791099,  0.59906078,  0.11461661,  1.33241763, -0.90442079,\n",
      "         0.63272797, -0.55744395, -0.88566065,  2.56404849, -1.41354882],\n",
      "       [ 1.06196688,  0.19778521, -0.03778408,  0.0404547 ,  0.35161172,\n",
      "         1.07767562, -1.28848774, -1.75058826, -1.18951244,  0.27749026,\n",
      "        -0.46402007,  0.1299233 ,  1.73366404, -0.78005905,  0.71315904,\n",
      "        -0.13778845, -1.45971481,  0.29821733,  0.43908994, -0.47409567],\n",
      "       [-0.24319286, -0.10717202,  0.09665543,  0.92429258, -0.55939398,\n",
      "        -0.75477169,  0.66699821,  1.16155298, -0.41540261,  0.64209848,\n",
      "         0.55819563,  0.02499968, -1.03424936, -0.22484992, -0.73194664,\n",
      "        -0.56791606,  0.37334298,  0.16412128,  0.86629545,  1.63828078],\n",
      "       [ 0.41994803, -0.17618544,  0.11235946,  0.67772755, -0.53241462,\n",
      "         1.80611402, -0.24978021,  0.29418363,  0.56862268,  0.54896175,\n",
      "         0.23550659, -1.34844028, -1.23184497,  0.31195169,  0.91359881,\n",
      "         1.98904193,  2.43104188, -0.92050977, -0.93931884, -1.24663605],\n",
      "       [-1.03338727, -0.19601893, -1.55384833,  2.58551616,  2.23075641,\n",
      "        -0.82079361,  1.24593675, -1.77053301,  0.41453196,  0.32293843,\n",
      "         2.13253496,  0.53024896, -1.22903758, -0.49039674,  1.26053196,\n",
      "        -0.7739857 , -0.71401863,  0.59828162, -0.12625878,  0.40798506],\n",
      "       [ 0.90538937, -0.43581825, -0.33297263, -0.81154749,  0.8098858 ,\n",
      "         0.03264615,  1.47361701, -0.41641977, -0.18084217,  0.29675644,\n",
      "        -1.28970503, -1.07241599,  1.1841741 , -2.29542188, -0.02396335,\n",
      "         1.44607352,  0.01798323, -0.16007067,  1.03573575,  1.7439358 ],\n",
      "       [-0.29452184, -0.02956378,  0.71031622, -0.07496262, -1.18536839,\n",
      "         0.33610044,  0.67411504,  0.07875396, -0.20580354, -1.98914169,\n",
      "        -0.18072535,  1.39144668, -1.84179736, -0.67941297,  0.14844094,\n",
      "        -0.35926143, -0.34583705, -1.18548444, -0.29476574,  0.37254828],\n",
      "       [ 1.17006993, -0.37494977, -0.41759032,  0.95351463,  1.95952198,\n",
      "         0.70271098, -0.67548028, -0.64892047, -0.24598671, -2.67327004,\n",
      "         0.23008451,  0.31275174,  1.00915663,  0.90687925,  0.46768178,\n",
      "        -1.0591593 ,  1.45960973, -0.06083548, -1.17432338, -1.92081216],\n",
      "       [ 0.44583257, -1.02394994,  0.72985106, -0.39016038,  1.05513564,\n",
      "        -1.04311492, -0.33363333, -1.39466917,  0.30547639,  0.25464916,\n",
      "        -1.10341971,  0.29567603, -0.40660571, -0.77695386, -0.99599723,\n",
      "         0.54378567, -1.71098555, -1.40828699, -2.03923648, -0.56490043],\n",
      "       [ 0.60705136,  0.18486633,  0.36261966,  1.25866584,  0.15123976,\n",
      "         0.0655832 , -0.21167839,  1.71118186,  0.69071337,  0.86127417,\n",
      "         0.87329852,  0.51573223,  0.65261808,  0.67693473,  0.77779375,\n",
      "         1.77739265,  1.0771842 ,  0.10743684,  1.20948626, -1.61230428],\n",
      "       [-0.64434194, -1.63518929, -1.39746872, -2.49582798, -0.07515894,\n",
      "        -0.82010452, -1.03410759,  1.62755666,  0.84866977,  0.20981864,\n",
      "        -1.42393098,  0.44696163, -0.14930902,  0.93044124, -1.04023347,\n",
      "         1.4913354 ,  0.37090928, -0.07900542, -0.05760143, -0.48129409]]), array([[ 1.43319449e+00,  6.53574098e-01, -2.16178367e-01,\n",
      "         1.96897389e+00,  5.24635654e-01, -9.05260447e-01,\n",
      "         2.24920523e-01,  7.28467710e-02, -2.01504188e-01,\n",
      "         7.56058733e-01],\n",
      "       [ 7.15470935e-01, -5.20422129e-01, -1.06891164e+00,\n",
      "        -2.80878884e-01, -7.41896009e-01,  5.28974622e-01,\n",
      "        -6.74424053e-01, -1.40736680e-02, -2.01804638e+00,\n",
      "        -3.15475901e-01],\n",
      "       [ 2.03436576e+00, -7.68993662e-01, -7.38589048e-01,\n",
      "        -5.34031957e-01,  9.86540550e-01, -9.31108363e-01,\n",
      "         1.77083071e+00, -1.86060487e+00, -5.63738335e-01,\n",
      "        -1.34123763e+00],\n",
      "       [-8.29507895e-02, -6.92453041e-01, -2.97366608e-01,\n",
      "         5.42670514e-02, -1.10941451e+00,  5.83473462e-01,\n",
      "        -2.92850252e+00, -3.10910028e-01,  3.48299558e-01,\n",
      "         6.77787505e-01],\n",
      "       [-1.15254093e+00, -1.31884642e+00, -7.26939081e-01,\n",
      "         5.16864235e-02, -9.83947644e-01,  1.03001177e+00,\n",
      "        -8.35830208e-01, -1.60144812e+00,  1.73299969e+00,\n",
      "         1.39592098e+00],\n",
      "       [ 2.03619645e-01,  1.12540048e+00,  7.30086309e-01,\n",
      "        -1.30205452e-01, -2.27642480e+00,  1.39055022e+00,\n",
      "         6.85043745e-01, -2.22077555e+00,  1.80531863e+00,\n",
      "        -5.12965730e-02],\n",
      "       [ 3.39358764e-01,  1.66092993e+00,  9.59799564e-01,\n",
      "         3.21388300e-01,  5.22073181e-01, -1.47963188e+00,\n",
      "        -7.39206947e-02, -9.04442908e-01, -7.74313403e-01,\n",
      "         2.22460731e+00],\n",
      "       [ 1.65153921e-01,  1.11321443e-01,  8.28490341e-01,\n",
      "        -8.34041919e-02, -1.24442121e+00, -9.60115439e-01,\n",
      "        -1.62974482e-01,  1.37637016e+00,  3.97899464e-02,\n",
      "        -8.70546313e-01],\n",
      "       [-2.41136006e-01, -8.97159208e-02,  8.47165731e-02,\n",
      "        -5.80236877e-01, -3.94568522e-01, -1.57437459e+00,\n",
      "        -2.69732373e-01, -1.67810861e+00,  9.72035042e-01,\n",
      "         1.19026779e+00],\n",
      "       [-6.31128964e-01,  2.35759372e-01, -6.79140502e-01,\n",
      "         6.28090973e-01,  6.97840628e-01, -9.19209020e-02,\n",
      "        -1.20952373e+00,  1.45674496e-01, -1.43535623e+00,\n",
      "         3.50177928e-01],\n",
      "       [-1.05797450e-01, -2.19771259e+00, -2.84406367e-02,\n",
      "         6.82749171e-01, -4.14568492e-01,  1.92712952e-01,\n",
      "        -6.82260517e-01,  1.27025040e+00,  2.58505617e-01,\n",
      "        -3.51077045e-01],\n",
      "       [ 1.67779953e-01,  1.33267189e+00, -4.60169966e-01,\n",
      "        -5.24585896e-01, -1.11555950e+00, -9.20916789e-04,\n",
      "        -4.80725141e-01,  1.40794101e+00, -3.86373713e-01,\n",
      "        -2.62834347e-01],\n",
      "       [ 1.09454821e-01,  6.42437315e-01,  5.89107335e-01,\n",
      "         1.48838496e+00,  1.78381723e-01,  1.65058198e+00,\n",
      "         1.67485153e+00, -9.17283327e-02,  1.67548536e-01,\n",
      "        -5.82403042e-01],\n",
      "       [ 1.99833305e+00, -9.70054755e-01,  3.33813249e-01,\n",
      "        -8.47047501e-02, -1.00687978e+00, -3.15699431e-01,\n",
      "         2.32384786e+00, -6.80130292e-01,  1.39083576e+00,\n",
      "         4.79680999e-01],\n",
      "       [-1.70174956e-01, -1.51555951e+00,  1.56377551e+00,\n",
      "        -7.12892960e-01, -1.48688277e+00, -8.85298924e-01,\n",
      "        -1.80228657e+00,  3.42170235e-01, -1.69013074e+00,\n",
      "         9.50894554e-01],\n",
      "       [-5.18489874e-01, -2.80964780e+00, -1.70000181e+00,\n",
      "         6.23088217e-01,  1.11517313e-01,  1.03342152e-01,\n",
      "         9.07791545e-01,  3.73524632e-01, -1.14101301e+00,\n",
      "        -1.67468140e+00],\n",
      "       [-1.24575769e+00, -8.37091958e-01,  1.27348803e+00,\n",
      "         1.56085238e+00, -3.09495874e+00, -3.17490114e-01,\n",
      "        -6.08013859e-01, -1.43673741e+00, -1.05824492e+00,\n",
      "        -1.40727015e+00],\n",
      "       [-1.61572759e+00,  4.54000761e-01, -6.88996274e-01,\n",
      "        -8.97233396e-01, -8.43281890e-01, -1.58285708e-01,\n",
      "        -4.55079678e-01, -7.02241805e-01, -4.56203311e-01,\n",
      "        -2.42547358e-01],\n",
      "       [-1.66985557e+00,  5.54132804e-02, -8.55638064e-01,\n",
      "        -2.87107244e-01,  4.10881499e-01,  3.92967465e-01,\n",
      "        -1.25064113e-01,  6.63036965e-01,  3.14112235e-01,\n",
      "        -2.83810242e-01],\n",
      "       [ 3.05431446e-02,  7.87484861e-01, -1.56668832e+00,\n",
      "        -2.02261037e+00, -6.59053244e-01,  1.82150058e-01,\n",
      "        -1.11177162e+00, -1.05395553e-01,  7.11425586e-01,\n",
      "         1.49156103e+00]])]\n",
      "The updates biases are [array([[ 0.63912669, -0.67441085,  0.21198863, -0.04324602,  1.22777797,\n",
      "         0.92498522,  0.78504888,  0.44859602,  1.77250113, -1.43832171,\n",
      "         0.37130264,  2.17869697,  0.07697812,  0.72206829,  1.07009654,\n",
      "        -0.92124393, -0.87019081,  0.16946604,  0.2232151 ,  0.97552227]]), array([[ 1.09740786, -0.25346187, -0.9451565 ,  2.20955576,  0.22739708,\n",
      "        -0.12826738,  0.3954373 , -0.57145536, -0.9435442 ,  0.59214882,\n",
      "        -1.35802501, -0.74940709, -0.20598395,  0.33831325,  1.68976515,\n",
      "         0.84114549,  0.17786379, -0.03890825,  1.35367368, -1.62409605]]), array([[ 0.13670557, -0.290113  ,  0.10361997, -1.03851657, -1.20003879,\n",
      "        -1.43119558,  0.20746785, -0.27350996, -0.74894757,  1.50877452]])]\n"
     ]
    }
   ],
   "source": [
    "def NeuralNet(x,y,weights,biases,lr,activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":   \n",
    "        \n",
    "        z1 = np.dot(x,weights[0]) + biases[0]     # z1 is (RxN)\n",
    "        a1 = sigmoid(z1)                          # a1 is (RxN)\n",
    "    \n",
    "        z2 = np.dot(a1,weights[1]) + biases[1]    # z2 is (RxN)\n",
    "        a2 = sigmoid(z2)                          # a2 is (RxN)\n",
    "\n",
    "        z3 = np.dot(a2, weights[2]) + biases[2]   # z3 is (Rx10)\n",
    "        a3 = softmax(z3)                          # a3 is (Rx10)\n",
    "\n",
    "        \n",
    "        a3_del = errordiff(a3, y_train)           # a3_del is (Rx10)\n",
    "        z2_delta = np.dot(a3_del, weights[2].T)   # z2_delta is (RxN)\n",
    "        \n",
    "        a2_delta = z2_delta *sigmoid_derv(a2)     # a2_delta is (RxN)\n",
    "        z1_delta = np.dot(a2_delta,weights[1].T)  # z1_delta is (RxN)\n",
    "        \n",
    "        a1_del = z1_delta*sigmoid_derv(a1)        # a1_del is (RxN)\n",
    "    \n",
    "        w3 = weights[2]- lr * np.dot(a2.T, a3_del)                  #w3 is (Nx10)\n",
    "        b3 = biases[2] - lr * np.sum(a3_del, axis=0, keepdims=True) #b3 is (1x10)\n",
    "        \n",
    "        w2 = weights[1]- lr * np.dot(a1.T,a2_delta)                   #w2 is (NxN)\n",
    "        b2 = biases[1]- lr * np.sum(a2_delta, axis=0, keepdims=True)  #b2 is (1x5)\n",
    "        \n",
    "        w1 = weights[0]- lr * np.dot(X.T, a1_del)                   #w1 is (784xN)\n",
    "        b1 = biases[0]- lr * np.sum(a1_del, axis=0)                 #b1 is (1xN)\n",
    "        \n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        weights.extend([w1,w2,w3])\n",
    "        biases.extend([b1,b2,b3])\n",
    "        \n",
    "        return weights,biases\n",
    "    \n",
    "    elif activation == \"tanh\":  \n",
    "        \n",
    "        z1 = np.dot(x, weights[0]) + biases[0]    # z1 is (RxN)\n",
    "        a1 = tanh(z1)                             # a1 is (RxN)\n",
    "        \n",
    "        z2 = np.dot(a1, weights[1]) + biases[1]   # z2 is (RxN)\n",
    "        a2 = tanh(z2)                             # a2 is (RxN)\n",
    "        \n",
    "        z3 = np.dot(a2, weights[2]) + biases[2]   # z3 is (Rx10)\n",
    "        a3 = softmax(z3)                          # a3 is (Rx10)\n",
    "        \n",
    "        a3_del = errordiff(a3, y_train)             # a3_del is (Rx10)\n",
    "        z2_delta = np.dot(a3_del, weights[2].T) # z2_delta is (RxN)\n",
    "        \n",
    "        a2_delta = z2_delta *tanhder(a2)          # a2_delta is (RxN)\n",
    "        z1_delta = np.dot(a2_delta,weights[1].T)  # z1_delta is (RxN)\n",
    "        \n",
    "        a1_del = z1_delta*tanhder(a1)           # a1_del is (RxN)\n",
    "        \n",
    "        \n",
    "        w3 = weights[2] - lr * np.dot(a2.T, a3_del)                 #w3 is (Nx10)\n",
    "        b3 = biases[2]- lr * np.sum(a3_del, axis=0, keepdims=True)  #b3 is (1x10)\n",
    "        \n",
    "        w2 = weights[1] -lr * np.dot(a1.T,a2_delta)                   #w2 is (NxN)\n",
    "        b2 = biases[1]- lr * np.sum(a2_delta, axis=0, keepdims=True)  #b2 is (1x5)\n",
    "        \n",
    "        w1 = weights[0] -lr * np.dot(x.T, a1_del)                   #w1 is (784xN)\n",
    "        b1 = biases[0]- lr * np.sum(a1_del, axis=0)                 #b1 is (1xN)\n",
    "        \n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        weights.extend([w1,w2,w3])\n",
    "        biases.extend([b1,b2,b3])\n",
    "        \n",
    "        return weights,biases\n",
    "    \n",
    "    elif activation == \"relu\":      \n",
    "        \n",
    "        z1 = np.dot(x,weights[0]) + biases[0]     # z1 is (RxN)\n",
    "        a1 = relu(z1)                             # a1 is (RxN) \n",
    "        \n",
    "        z2 = np.dot(a1,weights[1]) + biases[1]    # z2 is (RxN)\n",
    "        a2 = relu(z2)                             # a2 is (RxN) \n",
    "        \n",
    "        z3 = np.dot(a2, weights[2]) + biases[2]   # z3 is (Rx10)\n",
    "        a3 = softmax(z3)                          # a3 is (Rx10)\n",
    "        \n",
    "        \n",
    "        a3_del = errordiff(a3, y_train)             # a3_del is (Rx10)\n",
    "        z2_delta = np.dot(a3_del, weights[2].T) # z2_delta is (RxN)\n",
    "    \n",
    "        a2_delta = z2_delta*reluder(a2)           # a2_delta is (RxN)\n",
    "        z1_delta = np.dot(a2_delta,weights[1].T)  # z1_delta is (RxN)\n",
    "       \n",
    "        a1_del = z1_delta*reluder(a1)           # a1_del is (RxN)\n",
    "    \n",
    "        \n",
    "        w3 = weights[2] -lr * np.dot(a2.T, a3_del)                  #w3 is (Nx10)\n",
    "        b3 = biases[2]- lr * np.sum(a3_del, axis=0, keepdims=True)  #b3 is (1x10)\n",
    "        \n",
    "        w2 = weights[1]- lr * np.dot(a1.T,a2_delta)                   #w2 is (NxN)\n",
    "        b2 = biases[1]- lr * np.sum(a2_delta, axis=0, keepdims=True)  #b2 is (1x5)\n",
    "        \n",
    "        w1 = weights[0]- lr * np.dot(x.T, a1_del)                   #w1 is (784xN)\n",
    "        b1 = biases[0]- lr * np.sum(a1_del, axis=0)                 #b1 is (1xN)\n",
    "        \n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        weights.extend([w1,w2,w3])\n",
    "        biases.extend([b1,b2,b3])\n",
    "        \n",
    "        return weights,biases\n",
    "\n",
    "    \n",
    "results=NeuralNet(x_train,y_train,w,b,lr,'relu')   \n",
    "print('The updates weights are',results[0])\n",
    "print('The updates biases are',results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is identical to the previous one with just a momentum parameter that has been added and an epoch value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20    # neurons for hidden layers\n",
    "lr = 0.01 # learning rate\n",
    "\n",
    "w1 = np.random.randn(x_train.shape[1], N) \n",
    "b1 = np.random.randn(1,N) \n",
    "w2 = np.random.randn(N, N)\n",
    "b2 = np.random.randn(1, N) \n",
    "w3=  np.random.randn(N, 10)\n",
    "b3 = np.random.randn(1,10)\n",
    "\n",
    "w=[w1,w2,w3]\n",
    "b=[b1,b2,b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/(1 + np.exp(-s))\n",
    "\n",
    "def tanh(s):\n",
    "    return np.tanh(s)\n",
    "\n",
    "def relu(s):      \n",
    "    return np.maximum(0,s) \n",
    "\n",
    "def softmax(s):\n",
    "    exps = np.exp(s-  np.max(s))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def tanhder(s):\n",
    "    return (1-s**2)\n",
    "\n",
    "def reluder(s):\n",
    "    return 1.0*(s>0)\n",
    "\n",
    "def sigmoid_derv(x):\n",
    "    return x * (1 - x) \n",
    "\n",
    "def errordiff(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    res = pred - real\n",
    "    return res/n_samples\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updates weights are [array([[ 0.7912309 , -0.00858001, -0.83671896, ...,  0.71168652,\n",
      "         0.75705463,  1.07670403],\n",
      "       [ 1.28676713,  1.15841216, -2.3566597 , ...,  1.43663196,\n",
      "         0.73509768, -0.95398529],\n",
      "       [ 0.48931322,  0.1899332 ,  0.23887336, ..., -1.08585005,\n",
      "        -0.76292394, -0.08696089],\n",
      "       ...,\n",
      "       [ 0.12861635, -0.37682779,  0.5718788 , ..., -0.33556216,\n",
      "        -0.48094508, -0.34561233],\n",
      "       [ 0.29602554,  0.0513511 , -0.37836077, ...,  0.96476086,\n",
      "        -0.86233459, -1.0396994 ],\n",
      "       [ 1.01914873, -0.42949699,  0.92169447, ..., -1.48373662,\n",
      "        -1.0960656 , -0.55223783]]), array([[-1.44965606,  0.63931904,  1.22377172, -0.5576201 ,  0.73985256,\n",
      "        -0.51181977, -0.81679159,  1.58215133,  0.80864215, -0.60910377,\n",
      "        -0.38378825,  0.59580013, -3.0819262 ,  0.85642175,  0.42960341,\n",
      "         0.23519655, -0.72184062,  1.01094028,  0.48839639,  0.45682418],\n",
      "       [ 0.26899721,  1.08562868,  0.30556526, -0.42594275,  0.83583654,\n",
      "        -0.38210616,  0.20909579,  0.08130954,  0.03603992, -0.84500498,\n",
      "         2.62822514,  1.69062208,  0.7032127 , -0.7592352 , -0.20474629,\n",
      "        -0.80100671,  1.23945124,  2.37651041, -0.45178047, -0.44307387],\n",
      "       [-1.74449647, -0.30631388, -1.16414665,  0.43797407,  0.31633459,\n",
      "        -1.89080369, -0.84498842,  0.26982711, -0.78488554, -0.24212494,\n",
      "        -1.75971442,  0.19123591, -1.11368943, -1.36975581, -0.0759819 ,\n",
      "        -0.9563164 , -0.52824375, -0.80176304, -0.84817248, -1.68682812],\n",
      "       [ 0.16519764, -1.48410732,  1.24641239,  0.09299262,  0.31954096,\n",
      "        -2.84213603, -0.94326078, -1.39280107,  0.69215296, -2.19860597,\n",
      "         1.11451215,  0.30913623,  1.34215609,  0.06792213,  0.74851241,\n",
      "        -0.47078335,  2.37086178,  0.37215062, -1.04457184,  0.28414373],\n",
      "       [ 0.9065679 , -1.30202449, -1.28556594,  0.36707737, -0.67943136,\n",
      "        -1.37032385, -0.84219797,  0.52563676, -0.20257389, -0.78623859,\n",
      "         1.27530313, -0.69615803,  0.0609229 , -0.08836805,  0.07288567,\n",
      "         0.30656824,  0.92206071, -0.97117153,  0.22044841, -0.76397563],\n",
      "       [ 1.7558213 ,  0.65233902, -1.36308814,  1.25514144,  0.03293166,\n",
      "         1.83748165,  1.25482112, -0.44437872, -1.38622856, -1.96791471,\n",
      "         0.01573252,  1.73977821,  0.43296938, -1.32398706,  0.48157748,\n",
      "         0.08218827, -1.50621063, -0.69640672,  0.36819729, -0.64634232],\n",
      "       [-0.07444197, -0.28295833, -0.48886752, -1.52443107, -1.41885849,\n",
      "         1.61671772,  0.8044965 , -0.4849603 , -1.46513968,  1.10633731,\n",
      "         0.30404403, -0.0543703 , -0.83569228, -0.67488599, -1.46917738,\n",
      "         1.87290764,  0.13513745,  0.30723323, -1.61987121, -2.66406978],\n",
      "       [-0.20066307, -0.70778888,  0.23128144,  2.93572002,  1.54719338,\n",
      "        -0.75878817, -2.33155444,  3.07705537,  1.51095726, -2.56996046,\n",
      "        -1.42949321,  0.45060828, -0.33336894,  0.32578847,  0.35757125,\n",
      "        -0.06322124,  0.72290819,  0.96758686,  0.91632741,  1.09163454],\n",
      "       [ 0.80891402,  1.30411251,  0.4346572 , -2.48060199,  1.84632209,\n",
      "        -1.03510082,  1.43505932, -1.86481821,  0.28199913,  1.50137817,\n",
      "         0.33696252, -0.11161191, -0.74831055,  0.05165825,  1.56318904,\n",
      "        -0.74436992, -0.8305127 ,  0.36600431,  0.70141487, -0.28445788],\n",
      "       [ 0.21474147, -1.29977452, -0.85430463,  0.60075368,  0.47197097,\n",
      "         0.79948864,  0.38196917, -0.58157101,  0.85922212, -0.84843723,\n",
      "        -0.47140378,  0.25748961,  1.64448483,  0.41815777,  0.07354489,\n",
      "        -0.73832414, -0.55492274, -1.2821453 , -0.74934641,  1.08991342],\n",
      "       [-2.34889198,  1.46230963,  0.3670812 ,  1.11956003,  0.82052286,\n",
      "        -1.00602797, -0.63809161,  2.16900795,  0.69223989, -1.28912265,\n",
      "        -0.8785338 , -1.02099667,  0.17344024,  0.05111712,  0.91132334,\n",
      "         2.26647246,  1.14974724,  0.79064701, -0.69503529, -1.89624567],\n",
      "       [ 0.90411566, -0.39604331,  0.78201284, -0.28501885,  0.28997134,\n",
      "         0.99521741, -0.41098266,  0.14371823,  0.78600987, -0.15339707,\n",
      "         0.78548303,  0.17258894, -0.76699566,  0.20674494, -0.0445247 ,\n",
      "         0.67151813,  0.40858244, -1.03884384,  0.92649269, -0.12801492],\n",
      "       [-0.93739107, -0.78710959, -0.61441058,  1.44839799, -2.16151159,\n",
      "         0.02772904,  0.29878049,  1.18646611, -0.33565994,  1.88994778,\n",
      "         0.7239357 , -0.83808681, -0.24464072, -0.34520422,  0.72770796,\n",
      "        -0.73479675, -1.56302492,  0.60693148,  0.30800234, -0.24735966],\n",
      "       [ 0.35300393, -0.12794023,  2.07681452, -0.60100796, -0.19487953,\n",
      "        -0.80603974, -0.34248665,  0.42852203,  0.28472397,  0.56248634,\n",
      "        -0.09535258,  0.64199999,  0.98543604, -0.6274979 ,  1.64208388,\n",
      "         0.67780029,  0.12573371,  0.4815522 ,  0.32491702,  0.41924746],\n",
      "       [-0.07376603, -0.43549543,  0.19717106, -0.04413982,  0.01476   ,\n",
      "         0.55448849, -0.57697812, -0.87219967, -2.36766707,  1.05294797,\n",
      "        -1.20287598,  0.43396283, -0.80115626, -0.39502734, -2.139463  ,\n",
      "        -0.72159982, -0.41571397,  0.30392975,  0.09401004, -1.46742762],\n",
      "       [-1.49039538,  0.33153493, -0.8268371 ,  0.07009499,  0.02127403,\n",
      "        -0.01077494,  0.13846525,  0.47118197, -0.17861496,  0.35214373,\n",
      "         0.58856952, -0.69333163,  1.3165393 ,  1.16849168, -0.11944611,\n",
      "         0.97008906, -0.31918939, -0.97847399,  1.25756161,  0.24977303],\n",
      "       [ 0.16038592,  1.37646575,  0.70266661,  0.8717969 , -1.20633253,\n",
      "         0.06889531, -3.10690977, -0.28501091,  0.31069515, -0.4326295 ,\n",
      "         0.92279919, -0.21243676,  1.59733716, -0.30246012,  0.59764603,\n",
      "        -0.19927614, -1.5997883 , -0.95981278,  0.6224249 , -0.41346484],\n",
      "       [-0.76672986,  1.27678475, -1.38224891,  0.48634408,  0.88624835,\n",
      "         1.07780826, -0.51564842, -0.4123867 ,  0.72688277,  0.72760921,\n",
      "        -1.56915126, -1.47138496,  2.32358944, -1.49057657,  0.4854061 ,\n",
      "         0.82005796, -0.75597776,  0.15961555,  0.16724397,  0.37890318],\n",
      "       [-1.19900434, -0.73454735,  0.27320629,  2.32920018,  1.02134635,\n",
      "        -1.36491483, -0.36124494,  1.31954841, -0.85674257,  0.18785424,\n",
      "         1.52035763,  1.01575831, -0.7035294 ,  0.16257687, -0.80929468,\n",
      "         0.10066303,  0.05083474, -0.32428189, -1.31720745,  0.29561309],\n",
      "       [-0.42136527, -0.15853495,  0.06501686, -0.81395303,  0.77488538,\n",
      "        -1.48530461, -0.36159472,  0.6879054 , -1.10232168, -1.33687042,\n",
      "         0.70336043,  0.9669702 , -1.54894094, -0.49816204,  2.82066515,\n",
      "         0.29181003,  0.24639231,  0.32426651, -0.38599532,  0.83257039]]), array([[ 0.1683782 ,  0.80761564, -2.76696241,  0.10517063, -0.37652417,\n",
      "        -0.83025189, -0.41483923, -0.21750601, -0.74953376,  0.48500387],\n",
      "       [-0.9260193 , -2.16373301, -0.37272723,  0.20741148, -0.51650461,\n",
      "        -1.09053221,  1.67071   , -0.58034031, -1.1914837 , -0.47041229],\n",
      "       [ 1.96937229,  0.04726144,  0.33187405, -1.16999046,  0.08423589,\n",
      "        -0.73525436, -0.3172102 , -0.58949529, -0.12126268,  1.99172898],\n",
      "       [ 1.13720818, -0.21757681, -0.10842263, -0.64437046, -0.16596847,\n",
      "        -0.40643994, -1.08948141,  0.54792129,  0.90327262,  0.63121321],\n",
      "       [ 0.83716147,  1.19074111, -0.25984781,  0.49589582, -0.90260906,\n",
      "         0.29075002,  0.35172468, -0.22892378, -0.61659542, -0.57701243],\n",
      "       [-0.69909226,  0.88172912, -0.06471201, -1.19444748,  0.83623697,\n",
      "         2.10500332,  0.48854334,  0.54221568,  0.07468889, -0.54646443],\n",
      "       [-1.80325873, -0.69062838, -0.505622  ,  1.17433414, -0.96116493,\n",
      "        -1.34890309, -1.42197567, -0.09163014, -0.64354021, -0.73395744],\n",
      "       [-0.6415691 , -1.24340195, -2.00271819, -2.32855066, -0.86186786,\n",
      "        -0.76742365,  0.53655179, -0.66103245, -0.73551049,  0.04130478],\n",
      "       [ 0.15759659, -0.3101862 ,  1.62353341, -1.00353138, -0.84427705,\n",
      "        -0.33477528, -1.43043021,  1.72919544,  0.21552343, -1.40319224],\n",
      "       [ 1.00436745, -0.45436563, -0.0456201 ,  0.36196454, -0.08150428,\n",
      "        -1.06481238, -0.94407548, -0.06814205,  0.88865492,  0.29641595],\n",
      "       [ 0.1849943 , -0.29089517,  0.1771393 ,  0.4395265 ,  0.53931925,\n",
      "        -0.15870162, -1.00616001,  0.08139569, -1.04940784, -0.61473151],\n",
      "       [-1.05116032, -1.05881691, -2.11003817,  0.93262679, -0.10451111,\n",
      "         1.0017741 , -0.6564379 , -1.46932264,  0.92530741,  0.24057922],\n",
      "       [-0.02983756, -0.73246412, -0.42474373,  1.0717005 , -1.41760417,\n",
      "         0.18602826, -0.77818734, -1.3299298 ,  0.70925363, -1.30301763],\n",
      "       [ 0.59534526, -1.14689635,  1.13389171,  0.11147421, -0.17688215,\n",
      "         0.90940022,  0.9035265 , -2.09792169, -0.63873364, -1.25374983],\n",
      "       [ 1.29779396,  0.3137608 , -0.2681561 , -1.49169614, -1.9287021 ,\n",
      "        -0.38630176, -0.86831998,  2.59125694,  1.49361949,  0.19097851],\n",
      "       [ 0.03884718,  0.59507008, -1.78496104,  1.50707785,  0.44569247,\n",
      "         2.04736549, -0.61950113,  1.73216449,  0.43579916,  0.15910326],\n",
      "       [-1.54499185,  1.25458998, -0.28365245, -1.98596032, -0.68557339,\n",
      "        -0.78390416,  1.64767774,  0.10961627, -1.00243877,  1.37628239],\n",
      "       [ 0.65402783,  0.73212082,  0.19649564, -1.27374549,  0.95283503,\n",
      "        -0.52485726, -0.10852079, -0.94626238,  0.78454465, -1.02220409],\n",
      "       [-0.95063046, -2.17061201,  0.92974346, -0.05635699, -1.52236028,\n",
      "         0.26703805,  0.7213445 , -0.56671043,  0.49135025, -2.41120315],\n",
      "       [-0.20815373, -1.26958738,  0.45493992, -0.96474426,  2.67889424,\n",
      "        -0.25075716, -0.46353536,  0.60661353,  0.24394157, -0.99133486]])]\n",
      "The updates biases are [array([[-0.36479028, -0.10796083,  1.02893068,  3.19242733, -0.67292913,\n",
      "         0.46741818, -0.15435853, -0.89756113,  0.82512517, -0.47635172,\n",
      "        -0.73392687, -0.07237956, -0.06686261, -0.05268991, -1.63138809,\n",
      "         1.02512303, -1.18737897,  0.2497417 ,  1.6289247 , -2.08942935]]), array([[-0.04225252,  0.52358721, -1.0193037 ,  0.63803188,  0.80331147,\n",
      "        -0.69227369,  0.43550998, -1.68831645,  1.35659977, -0.55821839,\n",
      "         0.08191631, -0.16360267,  0.55291735, -0.27855425,  0.42328464,\n",
      "         0.56276226, -0.96520456, -0.06514967, -1.9955188 ,  0.45684527]]), array([[-3.50961285, -0.08944193,  2.42731149, -0.51642142,  1.39958407,\n",
      "        -1.71507767, -0.34095154,  0.13325023,  0.17657356,  0.85671455]])]\n"
     ]
    }
   ],
   "source": [
    "def NeuralNet(x,y,weights,biases,lr,activation,momentum,mf,epochs):\n",
    "    \n",
    "    if activation == \"sigmoid\":   \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            z1 = np.dot(x,weights[0]) + biases[0]     # z1 is (RxN)\n",
    "            a1 = sigmoid(z1)                          # a1 is (RxN)\n",
    "    \n",
    "            z2 = np.dot(a1,weights[1]) + biases[1]    # z2 is (RxN)\n",
    "            a2 = sigmoid(z2)                          # a2 is (RxN)\n",
    "\n",
    "            z3 = np.dot(a2, weights[2]) + biases[2]   # z3 is (Rx10)\n",
    "            a3 = softmax(z3)                          # a3 is (Rx10)\n",
    "\n",
    "        \n",
    "            a3_del = errordiff(a3, y_train)             # a3_del is (Rx10)\n",
    "            z2_delta = np.dot(a3_del, weights[2].T) # z2_delta is (RxN)\n",
    "        \n",
    "            a2_delta = z2_delta *sigmoid_derv(a2)     # a2_delta is (RxN)\n",
    "            z1_delta = np.dot(a2_delta,weights[1].T)  # z1_delta is (RxN)\n",
    "        \n",
    "            a1_del = z1_delta*sigmoid_derv(a1)      # a1_del is (RxN)\n",
    "        \n",
    "            w3 = weights[2]- lr * np.dot(a2.T, a3_del)+ mf * momentum[0]  #checked\n",
    "            b3 = biases[2] - lr * np.sum(a3_del, axis=0, keepdims=True)+ mf*momentum[3]  \n",
    "        \n",
    "            w2 = weights[1]- lr * np.dot(a1.T,a2_delta)+ mf * momentum[1]+mf*momentum[1]  \n",
    "            b2 = biases[1]- lr * np.sum(a2_delta, axis=0, keepdims=True)+ mf*momentum[4]  \n",
    "        \n",
    "            w1 = weights[0]- lr * np.dot(x.T, a1_del)+ mf * momentum[2]\n",
    "            b1 = biases[0]- lr * np.sum(a1_del, axis=0)+ mf*momentum[5]  \n",
    "            \n",
    "            momentum=[]\n",
    "            momentum.extend([- lr * np.dot(a2.T, a3_del),- lr * np.dot(a1.T,a2_delta),- lr * np.dot(x.T, a1_del),-lr*np.sum(a3_del, axis=0, keepdims=True),-lr*np.sum(a2_delta, axis=0, keepdims=True),-lr * np.sum(a1_del, axis=0)])\n",
    "        \n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        \n",
    "        weights.extend([w1,w2,w3])\n",
    "        biases.extend([b1,b2,b3])\n",
    "        \n",
    "        \n",
    "        return weights,biases\n",
    "    \n",
    "    elif activation == \"tanh\":  \n",
    "        \n",
    "        for i in range(epochs):\n",
    "        \n",
    "            z1 = np.dot(x, weights[0]) + biases[0]    # z1 is (RxN)\n",
    "            a1 = tanh(z1)                             # a1 is (RxN)\n",
    "        \n",
    "            z2 = np.dot(a1, weights[1]) + biases[1]   # z2 is (RxN)\n",
    "            a2 = tanh(z2)                             # a2 is (RxN)\n",
    "        \n",
    "            z3 = np.dot(a2, weights[2]) + biases[2]   # z3 is (Rx10)\n",
    "            a3 = softmax(z3)                          # a3 is (Rx10)\n",
    "        \n",
    "            a3_del = errordiff(a3, y_train)             # a3_del is (Rx10)\n",
    "        \n",
    "            z2_delta = np.dot(a3_del, weights[2].T) # z2_delta is (RxN)\n",
    "            a2_delta = z2_delta *tanhder(a2)          # a2_delta is (RxN)\n",
    "        \n",
    "            z1_delta = np.dot(a2_delta,weights[1].T)  # z1_delta is (RxN)\n",
    "            a1_del = z1_delta*tanhder(a1)           # a1_del is (RxN)\n",
    "        \n",
    "        \n",
    "            w3 = weights[2] - lr * np.dot(a2.T, a3_del)+ mf * momentum[0]                #w3 is (Nx10)\n",
    "            b3 = biases[2]- lr * np.sum(a3_del, axis=0, keepdims=True)+ mf * momentum[3] #b3 is (1x10)\n",
    "        \n",
    "            w2 = weights[1] -lr * np.dot(a1.T,a2_delta)+ mf * momentum[1]                  #w2 is (NxN)\n",
    "            b2 = biases[1]- lr * np.sum(a2_delta, axis=0, keepdims=True)+ mf * momentum[4] #b2 is (1x5)\n",
    "        \n",
    "            w1 = weights[0] -lr * np.dot(x.T, a1_del)+ mf * momentum[2]                  #w1 is (784xN)\n",
    "            b1 = biases[0]- lr * np.sum(a1_del, axis=0)+ mf * momentum[5]                #b1 is (1xN)\n",
    "            \n",
    "            momentum=[]\n",
    "            momentum.extend([- lr * np.dot(a2.T, a3_del),- lr * np.dot(a1.T,a2_delta),- lr * np.dot(x.T, a1_del),-lr*np.sum(a3_del, axis=0, keepdims=True),-lr*np.sum(a2_delta, axis=0, keepdims=True),-lr * np.sum(a1_del, axis=0)])\n",
    "        \n",
    "        \n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        \n",
    "        weights.extend([w1,w2,w3])\n",
    "        biases.extend([b1,b2,b3])\n",
    "        momentum.extend([- lr * np.dot(a2.T, a3_del),- lr * np.dot(a1.T,a2_delta),- lr * np.dot(x.T, a1_del),-lr*np.sum(a3_del, axis=0, keepdims=True),-lr*np.sum(a2_delta, axis=0, keepdims=True),-lr * np.sum(a1_del, axis=0)])\n",
    "        \n",
    "        return weights,biases\n",
    "    \n",
    "    elif activation == \"relu\":     \n",
    "        \n",
    "        for i in range(epochs):\n",
    "        \n",
    "            z1 = np.dot(x,weights[0]) + biases[0]     # z1 is (RxN)\n",
    "            a1 = relu(z1)                             # a1 is (RxN) \n",
    "        \n",
    "            z2 = np.dot(a1,weights[1]) + biases[1]    # z2 is (RxN)\n",
    "            a2 = relu(z2)                             # a2 is (RxN) \n",
    "        \n",
    "            z3 = np.dot(a2, weights[2]) + biases[2]   # z3 is (Rx10)\n",
    "            a3 = softmax(z3)                          # a3 is (Rx10)\n",
    "        \n",
    "        \n",
    "            a3_del = errordiff(a3, y_train)             # a3_del is (Rx10)\n",
    "        \n",
    "            z2_delta = np.dot(a3_del, weights[2].T) # z2_delta is (RxN)\n",
    "            a2_delta = z2_delta*reluder(a2)           # a2_delta is (RxN)\n",
    "        \n",
    "            z1_delta = np.dot(a2_delta,weights[1].T)  # z1_delta is (RxN)\n",
    "            a1_del = z1_delta*reluder(a1)           # a1_del is (RxN)\n",
    "    \n",
    "        \n",
    "            w3 = weights[2] -lr * np.dot(a2.T, a3_del)+ mf * momentum[0]                  #w3 is (Nx10)\n",
    "            b3 = biases[2]- lr * np.sum(a3_del, axis=0, keepdims=True)+ mf * momentum[3]  #b3 is (1x10)\n",
    "        \n",
    "            w2 = weights[1]- lr * np.dot(a1.T,a2_delta)+ mf * momentum[1]                   #w2 is (NxN)\n",
    "            b2 = biases[1]- lr * np.sum(a2_delta, axis=0, keepdims=True)+ mf * momentum[4]  #b2 is (1x5)\n",
    "        \n",
    "            w1 = weights[0]- lr * np.dot(x.T, a1_del) + mf * momentum[2]                  #w1 is (784xN)\n",
    "            b1 = biases[0]- lr * np.sum(a1_del, axis=0) + mf * momentum[5]                #b1 is (1xN)\n",
    "            \n",
    "            momentum=[]\n",
    "            momentum.extend([- lr * np.dot(a2.T, a3_del),- lr * np.dot(a1.T,a2_delta),- lr * np.dot(x.T, a1_del),-lr*np.sum(a3_del, axis=0, keepdims=True),-lr*np.sum(a2_delta, axis=0, keepdims=True),-lr * np.sum(a1_del, axis=0)])\n",
    "        \n",
    "\n",
    "        weights=[]\n",
    "        biases=[]\n",
    "        \n",
    "        weights.extend([w1,w2,w3])\n",
    "        biases.extend([b1,b2,b3])\n",
    "        \n",
    "        return weights,biases\n",
    "\n",
    "\n",
    "momentum=[0,0,0,0,0,0]   \n",
    "results=NeuralNet(x_train,y_train,w,b,lr,'relu',momentum,0.5,1000)   \n",
    "\n",
    "print('The updates weights are',results[0])\n",
    "print('The updates biases are',results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
